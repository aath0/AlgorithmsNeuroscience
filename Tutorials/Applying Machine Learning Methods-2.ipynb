{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial #5: Applying Machine Learning Methods to EEG Data on Group Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, same classification tasks in Tutorial-4 will be examined but this time analysis will be done on group level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset: \n",
    "The the previous tutorial data of the only one participant in 'Emotion-Antecedent Appraisal Checks: EEG and EMG data sets for Novelty and Pleasantness' is used. In this tutorial, all participans will be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Models\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-15.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-01.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-14.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-02.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-16.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-17.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-03.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-07.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-13.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-12.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-06.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-10.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-04.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-05.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-11.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-20.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-08.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-09.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-21.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-23.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-22.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-26.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-27.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-24.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n",
      "<ipython-input-2-524d89893e30>:10: RuntimeWarning: This filename (../../study1/study1_eeg/epochdata/P-18.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs = mne.read_epochs(files[f], verbose=False)\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "data_folder = '../../study1/study1_eeg/epochdata/'\n",
    "files = [data_folder+f for f in listdir(data_folder) if isfile(join(data_folder, f))]\n",
    "ids = [int(f[-6:-4]) for f in files]\n",
    "\n",
    "numberOfEpochs = np.zeros((len(ids), 3))\n",
    "# Read the EEG epochs:\n",
    "epochs_all_UN, epochs_all_UP, epochs_all_NP = [], [], []\n",
    "for f in range(len(files)):\n",
    "    epochs = mne.read_epochs(files[f], verbose=False)\n",
    "    epochs_UN = epochs['FU', 'FN']\n",
    "    epochs_UP = epochs['FU', 'FP']\n",
    "    epochs_NP = epochs['FN', 'FP']\n",
    "    numberOfEpochs[f,0] = int(len(epochs_UN.events))\n",
    "    numberOfEpochs[f,1] = int(len(epochs_UP.events))\n",
    "    numberOfEpochs[f,2] = int(len(epochs_NP.events))\n",
    "    UN, UP, NP = [ids[f]], [ids[f]], [ids[f]]\n",
    "    UN.append(epochs_UN)\n",
    "    UP.append(epochs_UP)\n",
    "    NP.append(epochs_NP)\n",
    "    epochs_all_UN.append(UN)\n",
    "    epochs_all_UP.append(UP)\n",
    "    epochs_all_NP.append(NP)\n",
    "\n",
    "#print(numberOfEpochs)\n",
    "epochs_all_UN = np.array(epochs_all_UN)\n",
    "epochs_all_UP = np.array(epochs_all_UP)\n",
    "epochs_all_NP = np.array(epochs_all_NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of epochs_UN: (25, 2)\n",
      "Shape of epochs_UP: (25, 2)\n",
      "Shape of epochs_NP: (25, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of epochs_UN: {}'.format(epochs_all_UN.shape))\n",
    "print('Shape of epochs_UP: {}'.format(epochs_all_UP.shape))\n",
    "print('Shape of epochs_NP: {}'.format(epochs_all_NP.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData_labels(epochs):\n",
    "    data, labels, ids = [], [], []\n",
    "    for p in epochs:\n",
    "        tmp_epoch = p[1]\n",
    "        tmp_labels = tmp_epoch.events[:,-1]\n",
    "        labels.extend(tmp_labels)\n",
    "        tmp_id = p[0]\n",
    "        ids.extend([tmp_id]*len(tmp_labels))\n",
    "        data.extend(tmp_epoch.get_data())\n",
    "        \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    ids = np.array(ids)\n",
    "    return data, labels, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #1:  Classification between Unpleasant and Pleasant Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_UP, labels_UP, ids_UP = getData_labels(epochs_all_UP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleMissingValues(data, labels):\n",
    "    for d in range(len(data)):\n",
    "        if np.all(np.isfinite(data[d])) == False:\n",
    "            print(d)\n",
    "            data[d] = np.nan_to_num(data[d])\n",
    "        if np.any(np.isnan(data[d])) == True:\n",
    "            data[d] = np.nan_to_num(data[d])\n",
    "    data = data.astype('float64')\n",
    "    labels = labels.astype('float64')\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_groupLevel(ids, predictions, labels):\n",
    "    print('ids:')\n",
    "    print(ids)\n",
    "    unique_ids = list(set(ids))\n",
    "    unique_ids.sort()\n",
    "    print('Unique Ids: ')\n",
    "    print(unique_ids)\n",
    "    results = []\n",
    "    for id in unique_ids:\n",
    "        indices = [i for i, x in enumerate(ids) if x == id]\n",
    "        res = 0\n",
    "        print('lenght of indices: ', len(indices))\n",
    "        for i in range(len(indices)):\n",
    "            if predictions[indices[i]] == labels[indices[i]]:\n",
    "                res += 1\n",
    "        print('res: ', str(res))\n",
    "        results.append(res/len(indices))\n",
    "    \n",
    "    return results, unique_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all models and then run cross validation for all of them for comparing their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis(solver='svd'))\n",
    "#Logistic Regression\n",
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(penalty='l1', random_state=42))\n",
    "\n",
    "models = [ clf_lr_pip] #, clf_lda_pip]\n",
    "model_names = [ 'LR'] #, 'LDA'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pinar/lib/python3.7/site-packages/numpy/core/fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/anaconda3/envs/pinar/lib/python3.7/site-packages/numpy/core/fromnumeric.py:90: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/anaconda3/envs/pinar/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1545: RuntimeWarning: overflow encountered in multiply\n",
      "  sqr = np.multiply(arr, arr, out=arr)\n",
      "/anaconda3/envs/pinar/lib/python3.7/site-packages/numpy/core/fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/anaconda3/envs/pinar/lib/python3.7/site-packages/numpy/core/fromnumeric.py:90: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/anaconda3/envs/pinar/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/envs/pinar/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data_UP, labels_UP = handleMissingValues(data_UP, labels_UP)\n",
    "results = []\n",
    "for i in range(len(models)):\n",
    "    print(model_names[i])\n",
    "    kfold = StratifiedKFold(n_splits=2, random_state=42)\n",
    "    if np.all(np.isfinite(data_UP)) == True and np.any(np.isnan(data_UP)) == False:\n",
    "        predictions_UP = cross_val_predict(models[i], data_UP, labels_UP, cv=kfold)\n",
    "        print('Predictions: ')\n",
    "        print(predictions_UP)\n",
    "        print('True labels: ')\n",
    "        print(labels_UP)\n",
    "        cv_accuracy, unique_ids = calculate_score_groupLevel(ids_UP, predictions_UP, labels_UP)\n",
    "        results.append(cv_accuracy)\n",
    "        print('Model ' + model_names[i] + ': ' + str(cv_accuracy))\n",
    "    else:\n",
    "        print('Data has infinite or NaN value!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plotModelComparison(results, model_names):\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Model Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    plt.show()\n",
    "#plotModelComparison(results, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plotCVScores_perParticipant(unique_ids, results, model_names):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.suptitle('CV accuracy Scores per Participant')\n",
    "    for i in range(len(results)):\n",
    "        ax.plot(ids, results[i], label=model_names[i])\n",
    "    plt.xlabel('Participant IDs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCVScores_perParticipant(unique_ids, results, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent t-test is applied to statistically show whether there is a significant difference between performace of two models. Small p-value means that performace of models are significantly different, large p-value indicates that models are performing similar. In general, as the threshold for determining whether two distributions are different or not p = 0.05 is preffered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyTTest(results, model_names):\n",
    "    if len(results) < 2:\n",
    "        print('Not enough values for t-test!')\n",
    "    else:\n",
    "        for i in range(len(results)):\n",
    "            for j in range(len(results)):\n",
    "                if i != j:\n",
    "                    t, p = stats.ttest_ind(results[i],results[j])\n",
    "                    print(\"p = {0} for t-test between {1} and {2}\".format(p,  model_names[i],  model_names[j])\n",
    "                    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applyTTest(results, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value is larger than 0.05, we can conclude that there is no significant difference between performance of LDA and performance of LR on the task of classification between unpleasant and pleasant events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #2:  Classification between Unpleasant and Pleasant Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with unpleasant and neutral events\n",
    "data_UP, labels_UP = getData_labels(epochs_all_UP)\n",
    "print(data_UP.shape)\n",
    "print(labels_UP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_UP, test_data_UP, labels_train_UP, labels_test_UP = train_test_split(data_UP, labels_UP, test_size=0.3, random_state=42)\n",
    "print(train_data_UP.shape)\n",
    "print(test_data_UP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis(solver='svd'))\n",
    "clf_lda_pip.fit(train_data_UP,labels_train_UP)\n",
    "\n",
    "predictions_lda = clf_lda_pip.predict(test_data_UP)\n",
    "\n",
    "acc_lda = accuracy_score(labels_test_UP, predictions_lda)\n",
    "print(\"Accuracy of LDA model: {}\".format(acc_lda))\n",
    "\n",
    "precision_lda,recall_lda,fscore_lda,support_lda=precision_recall_fscore_support(labels_test_UP,predictions_lda,average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lda,recall_lda,fscore_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(penalty='l1', random_state=42))\n",
    "clf_lr_pip.fit(train_data_UP,labels_train_UP)\n",
    "\n",
    "predictions_lr = clf_lr_pip.predict(test_data_UP)\n",
    "\n",
    "acc_lr = accuracy_score(labels_test_UP, predictions_lr)\n",
    "print(\"Accuracy of LR model: {}\".format(acc_lr))\n",
    "\n",
    "precision_lr,recall_lr,fscore_lr,support_lra=precision_recall_fscore_support(labels_test_UP,predictions_lr,average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lr,recall_lr,fscore_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies.append([acc_lda, acc_lr])\n",
    "f1_scores.append([fscore_lda, fscore_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #3: Classification between Pleasant and Neutral Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with unpleasant and neutral events\n",
    "data_NP, labels_NP = getData_labels(epochs_all_NP)\n",
    "print(data_NP.shape)\n",
    "print(labels_NP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_NP, test_data_NP, labels_train_NP, labels_test_NP = train_test_split(data_NP, labels_NP, test_size=0.3, random_state=42)\n",
    "print(train_data_NP.shape)\n",
    "print(test_data_NP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis(solver='svd'))\n",
    "clf_lda_pip.fit(train_data_NP,labels_train_NP)\n",
    "\n",
    "predictions_lda = clf_lda_pip.predict(test_data_NP)\n",
    "\n",
    "acc_lda = accuracy_score(labels_test_NP, predictions_lda)\n",
    "print(\"Accuracy of LDA model: {}\".format(acc_lda))\n",
    "\n",
    "precision_lda,recall_lda,fscore_lda,support_lda=precision_recall_fscore_support(labels_test_NP,predictions_lda,average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lda,recall_lda,fscore_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(penalty='l1', random_state=42))\n",
    "clf_lr_pip.fit(train_data_NP,labels_train_NP)\n",
    "\n",
    "predictions_lr = clf_lr_pip.predict(test_data_NP)\n",
    "\n",
    "acc_lr = accuracy_score(labels_test_NP, predictions_lr)\n",
    "print(\"Accuracy of LR model: {}\".format(acc_lr))\n",
    "\n",
    "precision_lr,recall_lr,fscore_lr,support_lr=precision_recall_fscore_support(labels_test_NP,predictions_lr,average='macro')\n",
    "print('Precision: {0}, Recall: {1}, f1-score:{2}'.format(precision_lr,recall_lr,fscore_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies.append([acc_lda, acc_lr])\n",
    "f1_scores.append([fscore_lda, fscore_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEvalMetrics(tasks, labels, evalMetric, metricName):\n",
    "    width = 0.2  # the width of the bars\n",
    "\n",
    "    # Set position of bar on X axis\n",
    "    rects1 = np.arange(len(evalMetric))\n",
    "    rects2 = [x + width for x in rects1]\n",
    "    rects3 = [x + width for x in rects2]\n",
    "\n",
    "    plt.bar(rects1, list(zip(*evalMetric))[0], color='#87CEFA', width=width, edgecolor='white', label=labels[0])\n",
    "    plt.bar(rects2, list(zip(*evalMetric))[1], color='#FFE4E1', width=width, edgecolor='white', label=labels[1])\n",
    "    \n",
    "    plt.xlabel('Classification Tasks')\n",
    "    plt.xticks([r + width/2 for r in range(len(evalMetric))], tasks)\n",
    "    plt.ylabel(metricName)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Accuracies\n",
    "tasks = ['UN', 'UP', 'NP']\n",
    "labels = ['LDA', 'LR']\n",
    "plotEvalMetrics(tasks, labels, accuracies, 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot F1 Scores\n",
    "tasks = ['UN', 'UP', 'NP']\n",
    "labels = ['LDA', 'LR']\n",
    "plotEvalMetrics(tasks, labels, f1_scores, 'F1-Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of group level analysis of eeg data, logistic regression (lr) and linear discriminant analysis (lda) are created as in the previous tutorial but this time svm is omitted because it requires hours to build with the amount of data we have. \n",
    "\n",
    "As both accuracy and f1 score plots demonstrated, logistic regression is performing better than lda on all tasks unlike the previous tutorial in which we analyze the data on individual participant level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
