{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial #5: Applying Machine Learning Methods to EEG Data on Group Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are performing the same classification tasks in Tutorial-4, but this time the analysis is done at the group level, considering EEG data from a group of participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset: \n",
    "The the previous tutorial data of the only one participant in 'Emotion-Antecedent Appraisal Checks: EEG and EMG data sets for Novelty and Pleasantness' is used. In this tutorial, data from all participans will be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import statistics\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Models\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "data_folder = '../../study1/study1_eeg/epochdata/'\n",
    "files = [data_folder+f for f in listdir(data_folder) if isfile(join(data_folder, f)) and '.DS_Store' not in f]\n",
    "ids = [int(f[-6:-4]) for f in files]\n",
    "\n",
    "numberOfEpochs = np.zeros((len(ids), 3))\n",
    "# Read the EEG epochs:\n",
    "epochs_all_UN, epochs_all_UP, epochs_all_NP = [], [], []\n",
    "for f in range(len(files)):\n",
    "    epochs = mne.read_epochs(files[f], verbose='error')\n",
    "    epochs_UN = epochs['FU', 'FN']\n",
    "    epochs_UP = epochs['FU', 'FP']\n",
    "    epochs_NP = epochs['FN', 'FP']\n",
    "    numberOfEpochs[f,0] = int(len(epochs_UN.events))\n",
    "    numberOfEpochs[f,1] = int(len(epochs_UP.events))\n",
    "    numberOfEpochs[f,2] = int(len(epochs_NP.events))\n",
    "    UN, UP, NP = [ids[f]], [ids[f]], [ids[f]]\n",
    "    UN.append(epochs_UN)\n",
    "    UP.append(epochs_UP)\n",
    "    NP.append(epochs_NP)\n",
    "    epochs_all_UN.append(UN)\n",
    "    epochs_all_UP.append(UP)\n",
    "    epochs_all_NP.append(NP)\n",
    "\n",
    "#print(numberOfEpochs)\n",
    "epochs_all_UN = np.array(epochs_all_UN)\n",
    "epochs_all_UP = np.array(epochs_all_UP)\n",
    "epochs_all_NP = np.array(epochs_all_NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of epochs_UN: (25, 2)\n",
      "Shape of epochs_UP: (25, 2)\n",
      "Shape of epochs_NP: (25, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of epochs_UN: {}'.format(epochs_all_UN.shape))\n",
    "print('Shape of epochs_UP: {}'.format(epochs_all_UP.shape))\n",
    "print('Shape of epochs_NP: {}'.format(epochs_all_NP.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that separates data, labels and the IDs of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData_labels(epochs):\n",
    "    data, labels, ids = [], [], []\n",
    "    for p in epochs:\n",
    "        tmp_epoch = p[1]\n",
    "        tmp_labels = tmp_epoch.events[:,-1]\n",
    "        labels.extend(tmp_labels)\n",
    "        tmp_id = p[0]\n",
    "        ids.extend([tmp_id]*len(tmp_labels))\n",
    "        data.extend(tmp_epoch.get_data())\n",
    "        \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    ids = np.array(ids)\n",
    "    return data, labels, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #1:  Classification between Unpleasant and Pleasant Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_UP, labels_UP, ids_UP = getData_labels(epochs_all_UP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard score calculation is done by simply calculating accuracy regardless of the participant. The following function calculates and returns accuracy per participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_groupLevel(ids, predictions, labels):\n",
    "    unique_ids = list(set(ids))\n",
    "    ids, predictions, labels = zip(*sorted(zip(ids, predictions, labels)))\n",
    "    #unique_ids.sort()\n",
    "    results = []\n",
    "    for id in unique_ids:\n",
    "        indices = [i for i, x in enumerate(ids) if x == id]\n",
    "        res = 0\n",
    "        for i in range(len(indices)):\n",
    "            if predictions[indices[i]] == labels[indices[i]]:\n",
    "                res += 1\n",
    "        results.append(res/len(indices))\n",
    "    \n",
    "    return results, unique_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create pipelines for all classifiers and run cross validation for all of them in order to compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis(solver='svd'))\n",
    "#Logistic Regression\n",
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(penalty='l1', random_state=42))\n",
    "\n",
    "models = [ clf_lr_pip, clf_lda_pip]\n",
    "model_names = [ 'LR', 'LDA'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyCrossValidation(models, model_names, ids, data, labels, kfold):\n",
    "    results, results_perParticipant, unique_ids_all = [], [], []\n",
    "    \n",
    "    if np.all(np.isfinite(data)) == True and np.any(np.isnan(data)) == False:\n",
    "        for i in range(len(models)):\n",
    "            print(model_names[i])\n",
    "            predictions = cross_val_predict(models[i], data, labels, cv=kfold)\n",
    "            cv_accuracy_perparticipant, unique_ids = calculate_score_groupLevel(ids, predictions, labels)\n",
    "            results_perParticipant.append(cv_accuracy_perparticipant)\n",
    "            unique_ids_all.append(unique_ids)\n",
    "            cv_accuracy = cross_val_score(models[i], data, labels, cv=kfold)\n",
    "            results.append(cv_accuracy)\n",
    "            print('CV accuracy of model ' + model_names[i] + ': ' + str(cv_accuracy))\n",
    "            print('CV accuracy of model ' + model_names[i] + ' per participant: ' + str(cv_accuracy_perparticipant))\n",
    "    else:\n",
    "        print('Data has infinite or NaN value!')\n",
    "    \n",
    "    return results, results_perParticipant, unique_ids_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, random_state=42)\n",
    "results, results_perParticipant, unique_ids_all = applyCrossValidation(models, model_names, ids_UP, data_UP, labels_UP, kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we caculate mean and standard deviation of cross validation scores for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_CVaccuracies, std_CVaccuracies = [], []\n",
    "print('Results UP: {}'.format(results))\n",
    "mean_CVaccuracies_UP, std_CVaccuracies_UP = [], []\n",
    "for i in range(len(results)):\n",
    "    mean_CVaccuracies_UP.append(statistics.mean(results[i]))\n",
    "    std_CVaccuracies_UP.append(statistics.stdev(results[i]))\n",
    "print('Mean cv accuracies UP: {}'.format(mean_CVaccuracies_UP))\n",
    "print('Std cv accuracies UP: {}'.format(std_CVaccuracies_UP))\n",
    "print('\\n')\n",
    "mean_CVaccuracies.append(mean_CVaccuracies_UP)\n",
    "std_CVaccuracies.append(std_CVaccuracies_UP)\n",
    "print('Mean cv accuracies: {}'.format(mean_CVaccuracies))\n",
    "print('Std cv accuracies: {}'.format(std_CVaccuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function plots lda and logistic regression scores for each partipant. As it can be observed that classifiers trained on all participants perform differently when are tested on each participant separately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plotCVScores_perParticipant(unique_ids, results, model_names):\n",
    "\n",
    "    # Fonts for the axis and title\n",
    "    title_font = {'fontname':'Arial', 'size':'24', 'color':'black', 'weight':'normal'} \n",
    "    axis_font = {'fontname':'Arial', 'size':'22'}\n",
    "    \n",
    "    width = 0.2  # the width of the bars\n",
    "    #participants = ['P-'+str(i) for i in unique_ids]\n",
    "    fig = plt.figure(num=None, figsize=(30, 10), dpi=150)\n",
    "    # Set position of bar on X axis\n",
    "    rects1 = np.arange(len(results[0]))\n",
    "    rects2 = [x + width for x in rects1]\n",
    "    \n",
    "    plt.bar(rects1, results[0][:], color='#87CEFA', width=width, edgecolor='white', label=model_names[0])\n",
    "    plt.bar(rects2, results[1][:], color='#FFE4E1', width=width, edgecolor='white', label=model_names[1])\n",
    "\n",
    "    plt.xticks([r + width/2 for r in range(len(unique_ids[0][:]))], unique_ids[0][:])\n",
    "    \n",
    "    plt.xlabel('Participant IDs', **axis_font)\n",
    "    plt.ylabel('Accuracy', **axis_font)\n",
    "    plt.title('CV Accuracy Scores per Participant', **title_font)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', prop={'size': 20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCVScores_perParticipant(unique_ids_all, results_perParticipant, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Box plots shows how a given dataset is distributed around the mean of the data and also they highlight outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plotModelComparison(results, model_names):\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Model Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    plt.show()\n",
    "    \n",
    "plotModelComparison(results, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical tests are applied to evaluate whether there is a significant difference between the performace of two classifiers. We choose Wilcoxon test which is a paired non-parametric test. Since the same data used for building and testing each classifier, a paired test like Wilcoxon can be applied in our case. We consider as significantly different classification performance where statistical tests give a p-value of p < 0.05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyStatisticalTest(results, model_names):\n",
    "    if len(results) < 2:\n",
    "        print('Not enough values for t-test!')\n",
    "    else:\n",
    "        for i in range(len(results)):\n",
    "            for j in range(i+1,len(results)):\n",
    "                t, p = stats.wilcoxon(results[i],results[j])\n",
    "                print(\"p = {0} for wilcoxon test between {1} and {2}\".format(p,  model_names[i],  model_names[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applyStatisticalTest(results, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the derived p-value is larger than 0.05, we can conclude that there is not a significant difference between performance of LDA and performance of LR on the task of classification between unpleasant and pleasant events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #2:  Classification between Unpleasant and Neutral Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with unpleasant and neutral events\n",
    "data_UN, labels_UN, ids_UN = getData_labels(epochs_all_UN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis(solver='svd'))\n",
    "#Logistic Regression\n",
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(penalty='l1', random_state=42))\n",
    "models = [ clf_lr_pip, clf_lda_pip]\n",
    "model_names = [ 'LR', 'LDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, random_state=42)\n",
    "results_UN, results_perParticipant_UN = applyCrossValidation(models, model_names, ids_UN, data_UN, labels_UN, kfold)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_UN)\n",
    "mean_CVaccuracies_UN, std_CVaccuracies_UN = [], []\n",
    "\n",
    "for i in range(len(results_UN)):\n",
    "    mean_CVaccuracies_UN.append(statistics.mean(results_UN[i]))\n",
    "    std_CVaccuracies_UN.append(statistics.stdev(results_UN[i]))\n",
    "\n",
    "mean_CVaccuracies.append(mean_CVaccuracies_UN)\n",
    "std_CVaccuracies.append(std_CVaccuracies_UN)\n",
    "print(mean_CVaccuracies)\n",
    "print(std_CVaccuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelComparison(results_UN, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applyTTest(results_UN, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar as above, since the derived p-value is smaller than 0.05, we can conclude that there is a significant difference between performance of LDA and performance of LR on the task of classification between unpleasant and neutral events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #3: Classification between Pleasant and Neutral Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with unpleasant and neutral events\n",
    "data_NP, labels_NP, ids_NP = getData_labels(epochs_all_NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "clf_lda_pip = make_pipeline(Vectorizer(), StandardScaler(), LinearDiscriminantAnalysis(solver='svd'))\n",
    "#Logistic Regression\n",
    "clf_lr_pip = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression(penalty='l1', random_state=42))\n",
    "models = [ clf_lr_pip, clf_lda_pip]\n",
    "model_names = [ 'LR', 'LDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=3, random_state=42)\n",
    "results_NP, results_perParticipant_NP = applyCrossValidation(models, model_names, ids_NP, data_NP, labels_NP, kfold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_NP)\n",
    "mean_CVaccuracies_NP, std_CVaccuracies_NP = [], []\n",
    "\n",
    "for i in range(len(results_NP)):\n",
    "    mean_CVaccuracies_NP.append(statistics.mean(results_NP[i]))\n",
    "    std_CVaccuracies_NP.append(statistics.stdev(results_NP[i]))\n",
    "\n",
    "mean_CVaccuracies.append(mean_CVaccuracies_NP)\n",
    "std_CVaccuracies.append(std_CVaccuracies_NP)\n",
    "print(mean_CVaccuracies)\n",
    "print(std_CVaccuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelComparison(results_NP, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applyTTest(results_NP, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Performance of Models Over Different Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEvalMetrics(tasks, labels, evalMetric, metricName):\n",
    "    width = 0.2  # the width of the bars\n",
    "\n",
    "    # Set position of bar on X axis\n",
    "    rects1 = np.arange(len(evalMetric))\n",
    "    rects2 = [x + width for x in rects1]\n",
    "    rects3 = [x + width for x in rects2]\n",
    "\n",
    "    plt.bar(rects1, list(zip(*evalMetric))[0], color='#87CEFA', width=width, edgecolor='white', label=labels[0])\n",
    "    plt.bar(rects2, list(zip(*evalMetric))[1], color='#FFE4E1', width=width, edgecolor='white', label=labels[1])\n",
    "    \n",
    "    plt.xlabel('Classification Tasks')\n",
    "    plt.xticks([r + width/2 for r in range(len(evalMetric))], tasks)\n",
    "    plt.ylabel(metricName)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Mean Accuracies\n",
    "tasks = ['UN', 'UP', 'NP']\n",
    "labels = ['LDA', 'LR']\n",
    "plotEvalMetrics(tasks, labels, mean_CVaccuracies, 'Mean CV Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Std of Accuracies\n",
    "tasks = ['UN', 'UP', 'NP']\n",
    "labels = ['LDA', 'LR']\n",
    "plotEvalMetrics(tasks, labels, std_CVaccuracies, 'Std of CV Accuracies ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of group level analysis of eeg data, LR and LDA are created as in the previous tutorial but this time SVM is omitted because it requires hours to compute given the large amount of data we have at group level. \n",
    "\n",
    "The first plot above showed the mean cross validation for both LDA and Logistic Regression on each task. While classifying between Unpleasant and Neutral events, both models provided accuracy around 50% which is the highest score among all tasks. For other two classification tasks, LDA performed better. It can be concluded that classification between Unpleasant and Pleasant events is the most difficult task for the models since both of them provided poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
